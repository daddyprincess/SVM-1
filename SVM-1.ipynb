{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80508ca6-faf5-42d4-b5b6-8631002a1525",
   "metadata": {},
   "source": [
    "## Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844c6ef-7a83-4dbf-a90e-109cb4fba9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) is based on the concept of finding the hyperplane that\n",
    "best separates two classes of data points. Here's the basic formula for a linear SVM:\n",
    "\n",
    "Given a dataset with features represented as vectors x_i and corresponding labels y_i (where y_i can be either -1 or +1 for\n",
    "binary classification):\n",
    "\n",
    "1.Objective Function:\n",
    "\n",
    "    ~The goal of a linear SVM is to find a hyperplane represented by a weight vector w and bias term b such that it maximizes \n",
    "      the margin between the two classes while minimizing classification errors. This is achieved by solving the following \n",
    "    optimization problem:\n",
    "\n",
    "Minimize:\n",
    "    \n",
    "    1/2 ||w||2\n",
    "\n",
    "Subject to:\n",
    "    \n",
    "    yi(w.xi+b)>1\n",
    "\n",
    "    ~Here, ||w|| represents the Euclidean norm (magnitude) of the weight vector w. The optimization problem seeks to minimize\n",
    "     the magnitude of w while ensuring that all data points are correctly classified with a margin of at least 1. The margin\n",
    "    is the distance between the hyperplane and the nearest data point from either class.\n",
    "\n",
    "2.Optimal Hyperplane:\n",
    "\n",
    "    ~The optimal hyperplane is found by solving the optimization problem, and it is represented by the weight vector w and \n",
    "     bias term b.\n",
    "\n",
    "3.Decision Function:\n",
    "\n",
    "The decision function for classification is determined by the sign of w · x + b:\n",
    "\n",
    "    ~If w · x + b is positive, the data point is classified as the +1 class.\n",
    "    ~If w · x + b is negative, the data point is classified as the -1 class.\n",
    "    \n",
    "In practice, the optimization problem is typically solved using quadratic programming or other optimization techniques. Once\n",
    "the optimal hyperplane is found, it can be used to classify new data points.\n",
    "\n",
    "The linear SVM aims to find the maximum-margin hyperplane that best separates the data, which results in a robust and often\n",
    "high-performing classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9a683-5c02-4cbb-b171-e40399e0d0a8",
   "metadata": {},
   "source": [
    "## Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848afdb-ebde-4d31-a097-6772e2b85a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is a mathematical expression that represents the optimization \n",
    "goal of finding the best hyperplane to separate two classes of data points while maximizing the margin between them. The\n",
    "objective function is used in the SVM's optimization problem. The primary objective of the SVM is to maximize the margin \n",
    "while minimizing classification errors. Here's the objective function for a linear SVM:\n",
    "\n",
    "Minimize:\n",
    "\n",
    "        1/2||w||2\n",
    "\n",
    "In this formula:\n",
    "\n",
    "    ~w is the weight vector that defines the orientation of the hyperplane.\n",
    "    ~||w|| represents the Euclidean norm (magnitude) of the weight vector w.\n",
    "The objective is to minimize the magnitude of the weight vector w. Minimizing ||w|| effectively maximizes the margin between \n",
    "the two classes. In other words, it finds the hyperplane that best separates the data points while maintaining a comfortable\n",
    "margin.\n",
    "\n",
    "Subject to this minimization objective, there is a set of constraints that ensure that all data points are correctly \n",
    "classified and lie on the correct side of the margin:\n",
    "\n",
    "Subject to:\n",
    "\n",
    "        yi(w.xi+b)>1\n",
    "\n",
    "Here:\n",
    "\n",
    "    ~y_i is the label of the i-th data point (either -1 or +1 for binary classification).\n",
    "    ~x_i is the feature vector of the i-th data point.\n",
    "    ~b is the bias term.\n",
    "The constraint ensures that each data point is correctly classified and has a margin of at least 1. If this constraint is\n",
    "satisfied for all data points, it means that the hyperplane effectively separates the classes and has a maximum margin.\n",
    "\n",
    "The linear SVM finds the optimal values of the weight vector w and bias term b that satisfy these constraints while\n",
    "minimizing the magnitude of w. This results in a hyperplane that not only separates the data but does so with the largest\n",
    "margin possible, making it robust and effective for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95bff3-9b69-44a2-abaa-e7f5ec81c507",
   "metadata": {},
   "source": [
    "## Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0b4201-4159-4a5e-954f-ef73d34aedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The kernel trick is a fundamental concept in Support Vector Machines (SVMs) that allows SVMs to handle nonlinearly separable\n",
    "data and perform complex pattern recognition tasks. It involves mapping the original feature space into a higher-dimensional\n",
    "space, where the data becomes linearly separable. The kernel trick is used to efficiently compute the dot product (inner\n",
    "product) between data points in this higher-dimensional space without explicitly calculating the transformed feature vectors.\n",
    "\n",
    "Here's a more detailed explanation of the kernel trick:\n",
    "\n",
    "1.Original Feature Space:\n",
    "\n",
    "    ~In a traditional linear SVM, the algorithm seeks to find a hyperplane in the original feature space that best separates\n",
    "     the two classes of data points.\n",
    "    ~However, in many real-world scenarios, the data may not be linearly separable in the original feature space.\n",
    "    \n",
    "2.Mapping to a Higher-Dimensional Space:\n",
    "\n",
    "    ~The kernel trick involves mapping the data from the original feature space (usually denoted as \"X\") into a higher-\n",
    "     dimensional space (often referred to as the \"feature space\" or \"Hilbert space\") where it becomes linearly separable.\n",
    "    ~This mapping is accomplished using a mathematical function called a \"kernel function.\"\n",
    "    \n",
    "3.Kernel Functions:\n",
    "\n",
    "    ~A kernel function, denoted as K(X, Y), computes the dot product (inner product) between two data points X and Y in the\n",
    "     higher-dimensional space, without explicitly calculating the feature vectors in that space.\n",
    "    ~Common kernel functions include the linear kernel (for linear separation), polynomial kernel, radial basis function\n",
    "     (RBF) kernel (commonly known as the Gaussian kernel), and more.\n",
    "        \n",
    "4.Support Vector Machine in Higher-Dimensional Space:\n",
    "\n",
    "    ~In the higher-dimensional space, the SVM algorithm finds the optimal hyperplane that best separates the data points.\n",
    "    ~Because of the mapping, this hyperplane can be nonlinear in the original feature space.\n",
    "5.Benefits:\n",
    "\n",
    "    ~The kernel trick allows SVMs to handle complex, nonlinear decision boundaries.\n",
    "    ~It avoids the need to explicitly compute and store the feature vectors in the higher-dimensional space, which would be\n",
    "     computationally expensive.\n",
    "    ~Instead, the kernel function efficiently computes the dot products as needed, making the approach feasible for high-\n",
    "     dimensional data.\n",
    "        \n",
    "6.Types of Kernel Functions:\n",
    "\n",
    "    ~Different kernel functions are suitable for different types of data and problems. For example, the Gaussian (RBF)\n",
    "     kernel is effective for capturing complex patterns, while the linear kernel is used when data is approximately linearly\n",
    "    separable.\n",
    "    ~The choice of kernel function is a crucial hyperparameter when using SVMs with the kernel trick.\n",
    "In summary, the kernel trick is a powerful technique that enables SVMs to handle nonlinear data by mapping it into a higher-\n",
    "dimensional space where linear separation is possible. This approach allows SVMs to perform complex pattern recognition\n",
    "tasks and is one of the key reasons for the popularity of SVMs in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f105c7-0297-4980-99ed-f93e757d3a5d",
   "metadata": {},
   "source": [
    "## Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986da930-28c7-40e4-af05-bcda6846180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support vectors are essential components of Support Vector Machines (SVMs) and play a crucial role in defining the decision\n",
    "boundary and the margin of the SVM classifier. They are the data points from the training set that are closest to the \n",
    "decision boundary, and they have a significant impact on the SVM's performance and the resulting decision boundary.\n",
    "\n",
    "Here's a more detailed explanation of the role of support vectors in SVMs, along with an example:\n",
    "\n",
    "Role of Support Vectors:\n",
    "\n",
    "1.Defining the Decision Boundary:\n",
    "\n",
    "    ~Support vectors are the data points that lie closest to the decision boundary or the hyperplane that separates\n",
    "     different classes in the feature space.\n",
    "    ~They are the most critical data points because they contribute the most to the determination of the decision boundary.\n",
    "    \n",
    "2.Margin Maximization:\n",
    "\n",
    "    ~The goal of SVMs is to maximize the margin, which is the distance between the decision boundary and the nearest data \n",
    "     points (support vectors) from both classes.\n",
    "    ~By maximizing the margin, SVMs aim to find a robust decision boundary that generalizes well to new, unseen data.\n",
    "    \n",
    "3.Support Vectors and Margin Constraints:\n",
    "\n",
    "    ~The position of support vectors relative to the decision boundary is crucial. They define the margin and the margin\n",
    "     constraints.\n",
    "\n",
    "Support vectors are the data points for which the following condition holds:\n",
    "\n",
    "\n",
    "                y_i(w · x_i + b) = 1\n",
    "\n",
    "        ~y_i is the label (+1 or -1) of the i-th data point.\n",
    "        ~w is the weight vector.\n",
    "        ~x_i is the feature vector of the i-th data point.\n",
    "        ~b is the bias term.\n",
    "        \n",
    "The margin constraints state that the product of the label y_i and the distance of the support vector x_i to the decision \n",
    "boundary (w · x_i + b) must be equal to 1. This implies that the support vectors lie exactly on the margin.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a simple binary classification example with two classes, denoted as class A and class B. The dataset consists\n",
    "of two features, and we want to build a linear SVM classifier to separate these two classes.\n",
    "\n",
    "    ~In the figure below, the circles represent data points from class A, and the squares represent data points from class B.\n",
    "\n",
    "    ~The line in the middle is the decision boundary (hyperplane) found by the SVM. The support vectors are highlighted;\n",
    "     these are the data points that are closest to the decision boundary and lie exactly on the margin.\n",
    "\n",
    "    ~The margin is represented by the two dashed lines, with support vectors from both classes lying on these lines.\n",
    "\n",
    "Support Vectors Example\n",
    "\n",
    "In this example, the support vectors (one from class A and one from class B) define the margin, the position of the decision\n",
    "boundary, and the overall behavior of the SVM classifier. Removing any other data points from the training set would not \n",
    "significantly affect the position of the decision boundary or the margin. Support vectors are critical for the SVM's ability\n",
    "to generalize and make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d752c-da8d-4724-9024-9b07af6503d2",
   "metadata": {},
   "source": [
    "## Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77203e1d-d162-4a0d-9874-3c394e0fc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "To illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines (SVM), \n",
    "let's consider a simple 2D binary classification problem with two classes, class A and class B. We'll use example data \n",
    "points and graphical representations for each concept.\n",
    "\n",
    "Example Data:\n",
    "\n",
    "Let's assume the following example data points for class A (circles) and class B (squares) in a 2D feature space:\n",
    "\n",
    "Class A:\n",
    "A1 = (2, 2)\n",
    "A2 = (3, 3)\n",
    "A3 = (4, 3)\n",
    "\n",
    "Class B:\n",
    "B1 = (1, 1)\n",
    "B2 = (2, 1)\n",
    "B3 = (3, 2)\n",
    "\n",
    "\n",
    "Graphical Representation:\n",
    "\n",
    "Here's a graphical representation of the data points:\n",
    "\n",
    "SVM Example Data\n",
    "\n",
    "Now, let's explore the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM:\n",
    "\n",
    "1.Hyperplane:\n",
    "\n",
    "    ~The hyperplane is the decision boundary that separates the two classes. In a 2D feature space, the hyperplane is a line.\n",
    "    ~It is determined by the weight vector w and the bias term b in the SVM formulation.\n",
    "    ~The goal of SVM is to find the hyperplane that best separates the classes.\n",
    "\n",
    "2.Marginal Plane:\n",
    "\n",
    "    ~The marginal plane refers to the lines parallel to the hyperplane that touch/support the closest data points from both\n",
    "     classes. These closest data points are the support vectors.\n",
    "    ~These marginal planes define the margin of the SVM.\n",
    "\n",
    "3.Hard Margin:\n",
    "\n",
    "    ~In a hard margin SVM, the goal is to find a hyperplane that separates the classes with the largest possible margin \n",
    "     while ensuring that all data points are correctly classified.\n",
    "    ~Hard margin SVM is suitable when the data is perfectly separable.\n",
    "\n",
    "4.Soft Margin:\n",
    "\n",
    "    ~In a soft margin SVM, the goal is to find a hyperplane that allows for some misclassification (classification errors) \n",
    "     to achieve a larger margin.\n",
    "    ~Soft margin SVM is suitable when the data is not perfectly separable or when there are outliers.\n",
    "    ~It introduces a parameter C that controls the trade-off between maximizing the margin and minimizing misclassification.\n",
    "\n",
    "\n",
    "In the soft margin example, we see that the SVM allows for a small amount of misclassification (circles from class A and \n",
    "squares from class B) to achieve a larger margin. The parameter C influences the balance between the margin size and the\n",
    "number of misclassified points.\n",
    "\n",
    "In summary, the concepts of hyperplane, marginal plane, soft margin, and hard margin are fundamental in SVMs for defining\n",
    "decision boundaries, maximizing margins, and handling different types of datasets. The choice between hard and soft margin \n",
    "depends on the nature of the data and the trade-off between margin size and misclassification tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d17f3-a479-41c8-b027-ef80202a146c",
   "metadata": {},
   "source": [
    "## Q6. SVM Implementation through Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8717113-38c7-4745-9111-2dd1bd116c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure, I can help you implement a linear SVM classifier using the Iris dataset in Python. We'll first use the scikit-learn\n",
    "implementation and then implement a simple linear SVM from scratch. We'll also explore how different values of the\n",
    "regularization parameter C affect the model's performance.\n",
    "\n",
    "Part 1: Using scikit-learn\n",
    "\n",
    "Here's how you can perform the tasks using scikit-learn:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1)  # You can try different values of C\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.title('SVM Decision Boundaries (C=1)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "This code will load the Iris dataset, split it into a training set and a testing set, train a linear SVM classifier, compute\n",
    "the accuracy, and visualize the decision boundaries. You can try different values of the regularization parameter C to\n",
    "observe their effect on the decision boundaries and accuracy.\n",
    "\n",
    "Part 2: Implementing a Linear SVM from Scratch\n",
    "\n",
    "Implementing a linear SVM from scratch is a more involved process. I'll provide a simplified version for illustration:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 2)\n",
    "y = np.where(X[:, 0] + X[:, 1] > 1, 1, -1)\n",
    "\n",
    "# Define the SVM training function\n",
    "def svm_train(X, y, learning_rate=0.01, epochs=1000, reg_param=0.01):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, x_i in enumerate(X):\n",
    "            condition = y[i] * (np.dot(x_i, weights) - bias) >= 1\n",
    "            if condition:\n",
    "                weights -= learning_rate * (2 * reg_param * weights)\n",
    "            else:\n",
    "                weights -= learning_rate * (2 * reg_param * weights - np.dot(x_i, y[i]))\n",
    "                bias -= learning_rate * y[i]\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Train the SVM\n",
    "weights, bias = svm_train(X, y)\n",
    "\n",
    "# Make predictions\n",
    "def predict(X, weights, bias):\n",
    "    return np.sign(np.dot(X, weights) - bias)\n",
    "\n",
    "# Generate a grid of points for decision boundary plotting\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = predict(np.c_[xx.ravel(), yy.ravel()], weights, bias)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM Decision Boundary (Custom Implementation)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "This code demonstrates a simplified linear SVM implementation from scratch using toy data. The training function svm_train \n",
    "trains the SVM, and the predict function makes predictions. You can adjust the learning rate, regularization parameter, and\n",
    "number of epochs to see how they affect the decision boundary.\n",
    "\n",
    "Keep in mind that this is a basic implementation for educational purposes and may not be as efficient or robust as\n",
    "professional libraries like scikit-learn for real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
